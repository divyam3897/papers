# [Neural Machine Translation in Linear Time](https://arxiv.org/pdf/1610.10099.pdf)

## Summary
* Applies a Wavenet similar architecture to the task of Machine Translation. 
* Encoder and Decoder are CNNs that use Dilated Convolutions and they are stacked on top of each other. 
* The Target Network uses Masked convolutions to ensure that it only relies on information from the past.

### Architecture:
* Dynamic Unfolding. The goal of dynamic unfolding is to accomodate source and target sequences of different lengths. To do this, the source network starts by generating a string representation that has the same length as the source sequence. Then, anything beyond the length of the source sequence is zero-padded. 
* Masked 1D Convolutions: Ensures that information from future tokens does not affect the prediction of the current token.
* Dilation: The dilation scheme is doubled at every layer starting from 1 and capped at 16 (i.e. 1, 2, 4, 8 and 16).
* Residual Blocks. 2 variants of residual blocks are wrapped around each layer. ReLU for machine translation and Multiplicative Units for language modeling.
* Word Embedding. Bag of n-gram character is used for the translation task.

## Strengths
* Linear runtime - up to a constant
* Parallelizable training and decoding.
* Resolution preserving - the size of the representation generated by the source network is proportional to the amount of information it represents and predicts later on.

